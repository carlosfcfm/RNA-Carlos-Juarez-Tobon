{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcde703",
   "metadata": {},
   "source": [
    "## PARTE 1: CREACIÃ“N Y ENTRENAMIENTO DEL MODELO CONVOLUCIONAL 2D BASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ad17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 162770 validated image filenames.\n",
      "Found 19867 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LamdaZero\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025/11/29 03:26:55 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'keras.src.legacy.preprocessing.image.DataFrameIterator'>. Dataset logging skipped.\n",
      "2025/11/29 03:26:55 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'keras.src.legacy.preprocessing.image.DataFrameIterator'>. Dataset logging skipped.\n",
      "c:\\Users\\LamdaZero\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - binary_accuracy: 0.8036 - loss: 0.4474\n",
      "Epoch 1: val_loss improved from None to 0.38841, saving model to C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES\\celeba_attribute_model_improved.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m992s\u001b[0m 194ms/step - binary_accuracy: 0.8127 - loss: 0.4263 - val_binary_accuracy: 0.8312 - val_loss: 0.3884 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - binary_accuracy: 0.8294 - loss: 0.3897\n",
      "Epoch 2: val_loss improved from 0.38841 to 0.35053, saving model to C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES\\celeba_attribute_model_improved.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m991s\u001b[0m 195ms/step - binary_accuracy: 0.8332 - loss: 0.3801 - val_binary_accuracy: 0.8456 - val_loss: 0.3505 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - binary_accuracy: 0.8399 - loss: 0.3621\n",
      "Epoch 3: val_loss improved from 0.35053 to 0.34458, saving model to C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES\\celeba_attribute_model_improved.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m996s\u001b[0m 196ms/step - binary_accuracy: 0.8424 - loss: 0.3571 - val_binary_accuracy: 0.8610 - val_loss: 0.3446 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - binary_accuracy: 0.8488 - loss: 0.3450\n",
      "Epoch 4: val_loss did not improve from 0.34458\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m980s\u001b[0m 193ms/step - binary_accuracy: 0.8512 - loss: 0.3411 - val_binary_accuracy: 0.8703 - val_loss: 0.4817 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m   3/5086\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m15:20\u001b[0m 181ms/step - binary_accuracy: 0.8582 - loss: 0.3274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LamdaZero\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss did not improve from 0.34458\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - binary_accuracy: 0.8635 - loss: 0.3182 - val_binary_accuracy: 0.8706 - val_loss: 0.4655 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - binary_accuracy: 0.8558 - loss: 0.3301\n",
      "Epoch 6: val_loss did not improve from 0.34458\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m975s\u001b[0m 192ms/step - binary_accuracy: 0.8570 - loss: 0.3272 - val_binary_accuracy: 0.8741 - val_loss: 0.4987 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - binary_accuracy: 0.8605 - loss: 0.3202\n",
      "Epoch 7: val_loss did not improve from 0.34458\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m986s\u001b[0m 194ms/step - binary_accuracy: 0.8611 - loss: 0.3182 - val_binary_accuracy: 0.8736 - val_loss: 0.4069 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - binary_accuracy: 0.8645 - loss: 0.3104\n",
      "Epoch 8: val_loss improved from 0.34458 to 0.29666, saving model to C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES\\celeba_attribute_model_improved.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1014s\u001b[0m 199ms/step - binary_accuracy: 0.8648 - loss: 0.3097 - val_binary_accuracy: 0.8837 - val_loss: 0.2967 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - binary_accuracy: 0.8672 - loss: 0.3041\n",
      "Epoch 9: val_loss improved from 0.29666 to 0.27346, saving model to C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES\\celeba_attribute_model_improved.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1020s\u001b[0m 201ms/step - binary_accuracy: 0.8672 - loss: 0.3036 - val_binary_accuracy: 0.8820 - val_loss: 0.2735 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m   3/5086\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m16:39\u001b[0m 197ms/step - binary_accuracy: 0.8672 - loss: 0.3002\n",
      "Epoch 10: val_loss improved from 0.27346 to 0.27234, saving model to C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES\\celeba_attribute_model_improved.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12ms/step - binary_accuracy: 0.8687 - loss: 0.3055 - val_binary_accuracy: 0.8825 - val_loss: 0.2723 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - binary_accuracy: 0.8684 - loss: 0.3003\n",
      "Epoch 11: val_loss did not improve from 0.27234\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 193ms/step - binary_accuracy: 0.8686 - loss: 0.3000 - val_binary_accuracy: 0.8437 - val_loss: 2.7506 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - binary_accuracy: 0.8696 - loss: 0.2980\n",
      "Epoch 12: val_loss did not improve from 0.27234\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m986s\u001b[0m 194ms/step - binary_accuracy: 0.8697 - loss: 0.2972 - val_binary_accuracy: 0.8673 - val_loss: 0.3118 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - binary_accuracy: 0.8736 - loss: 0.2877\n",
      "Epoch 13: val_loss did not improve from 0.27234\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m994s\u001b[0m 196ms/step - binary_accuracy: 0.8749 - loss: 0.2849 - val_binary_accuracy: 0.8880 - val_loss: 0.2953 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - binary_accuracy: 0.8762 - loss: 0.2820\n",
      "Epoch 14: val_loss did not improve from 0.27234\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1003s\u001b[0m 197ms/step - binary_accuracy: 0.8762 - loss: 0.2816 - val_binary_accuracy: 0.8913 - val_loss: 0.3244 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m   3/5086\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m15:30\u001b[0m 183ms/step - binary_accuracy: 0.8839 - loss: 0.2536\n",
      "Epoch 15: val_loss did not improve from 0.27234\n",
      "\u001b[1m5086/5086\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 5ms/step - binary_accuracy: 0.8812 - loss: 0.2667 - val_binary_accuracy: 0.8913 - val_loss: 0.3473 - learning_rate: 2.0000e-04\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run bald-roo-499 at: https://dagshub.com/carlosfcfm/Experimentos.mlflow/#/experiments/22/runs/b3f5f187a9384503a63d9ebbab1bf5a5\n",
      "ğŸ§ª View experiment at: https://dagshub.com/carlosfcfm/Experimentos.mlflow/#/experiments/22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "import mlflow\n",
    "from getpass import getpass\n",
    "\n",
    "# ############### Inicializamos conectividad con el repositorio \"Experimentos\" en DagsHub ###########\n",
    "REPO_NAME = \"Experimentos\"\n",
    "REPO_OWNER = \"carlosfcfm\"  # Escribir nombre de repositorio\n",
    "USER_NAME = \"carlosfcfm\"  # Escribir su usuario\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = USER_NAME\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token or password: ')\n",
    "mlflow.set_tracking_uri(f'https://dagshub.com/{REPO_OWNER}/{REPO_NAME}.mlflow')\n",
    "\n",
    "# Rutas\n",
    "base_dir = r'C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES'\n",
    "img_dir = os.path.join(base_dir, 'img_align_celeba')\n",
    "attr_csv = os.path.join(base_dir, 'list_attr_celeba.csv')\n",
    "partition_csv = os.path.join(base_dir, 'list_eval_partition.csv')\n",
    "\n",
    "# Cargamos los CSVs\n",
    "attributes = pd.read_csv(attr_csv)\n",
    "partitions = pd.read_csv(partition_csv)\n",
    "data = attributes.merge(partitions, on='image_id')\n",
    "\n",
    "# Mapeamos -1 a 0 para etiquetas binarias\n",
    "attr_columns = data.columns[1:41]  # Seleccionamos las 40 columnas de atributos\n",
    "data[attr_columns] = (data[attr_columns] + 1) // 2  # De -1/1 a 0/1\n",
    "\n",
    "# Filtramos el train (0) y val (1)\n",
    "train_data = data[data['partition'] == 0]\n",
    "val_data = data[data['partition'] == 1]\n",
    "# En este caso el dataset ya viene estratificado, por eso mismo no necesito hacerlo por mi cuenta\n",
    "# Tiene aproximadamente el 80% en train, 10% validaciÃ³n y 10% train\n",
    "# Generadores de datos con augmentaciÃ³n para train\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.6, 1.4],\n",
    "    channel_shift_range=0.1,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# FunciÃ³n para generar flow_from_dataframe\n",
    "def create_generator(datagen, df, batch_size=8, target_size=(96, 96)):\n",
    "    return datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        directory=img_dir,\n",
    "        x_col='image_id',\n",
    "        y_col=attr_columns.tolist(),\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw' \n",
    "    )\n",
    "\n",
    "train_generator = create_generator(train_datagen, train_data)\n",
    "val_generator = create_generator(val_datagen, val_data)\n",
    "\n",
    "# Modelo convolucional 2D\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), activation='relu', input_shape=(96,96,3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(512, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(512, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(40, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# #### Setup y entrenamiento del modelo con logging de MLflow y callbacks ################################\n",
    "filepath = os.path.join(base_dir, 'celeba_attribute_model_improved.keras')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, patience=5, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_binary_accuracy', factor=0.2, patience=4, min_lr=1e-7, verbose=1)\n",
    "\n",
    "mlflow.set_experiment(\"experimentos_celebA\")  \n",
    "mlflow.start_run(nested=True)\n",
    "mlflow.tensorflow.autolog(log_models=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=AdamW(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_data) // 32,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_data) // 32,\n",
    "    epochs=50,  \n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "model.save(filepath)\n",
    "mlflow.log_artifact(filepath, artifact_path=\"models\")\n",
    "mlflow.end_run()\n",
    "model.save(os.path.join(base_dir, 'celeba_attribute_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3264c69",
   "metadata": {},
   "source": [
    "## PARTE 2: GENERADOR Y AUMENTO DE IMAGENES DEL DATASET (TRAIN) DE MI ROSTRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AugmentaciÃ³n completada. ImÃ¡genes guardadas en augmented_my_faces.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Rutas\n",
    "base_dir = r'C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES'\n",
    "my_faces_dir = os.path.join(base_dir, 'my_faces')\n",
    "augmented_dir = os.path.join(base_dir, 'augmented_my_faces')\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "\n",
    "# DataGenerator para aumentar artificialmente las imagenes\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.6, 1.4]\n",
    ")\n",
    "\n",
    "# Para cada imagen en la carpeta my_faces que contiene 52 imagenes\n",
    "for filename in os.listdir(my_faces_dir):\n",
    "    if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "        img_path = os.path.join(my_faces_dir, filename)\n",
    "        img = load_img(img_path)  # carga imagen\n",
    "        x = img_to_array(img)  # convertimos a array\n",
    "        x = x.reshape((1,) + x.shape)  # Batch de 1\n",
    "\n",
    "        # Generamos 100 augmentadas por original \n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1, save_to_dir=augmented_dir, save_prefix='aug_' + filename.split('.')[0], save_format='jpeg'):\n",
    "            i += 1\n",
    "            if i > 10:\n",
    "                break\n",
    "\n",
    "print(\"AugmentaciÃ³n completada. ImÃ¡genes guardadas en augmented_my_faces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10f0e0",
   "metadata": {},
   "source": [
    "## PARTE 3: HACEMOS TRANSFER-LEARNING CON OTRO CLASIFICADOR Y CONGELANDO LOS PESOS DEL MODELO BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632953cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃºmero de negativos similares disponibles: 572\n",
      "No hay suficientes similaresâ€”usando todos + aleatorios extras\n",
      "DistribuciÃ³n train: {0.0: 915, 1.0: 457}\n",
      "DistribuciÃ³n val:   {0.0: 229, 1.0: 115}\n",
      "Found 1372 validated image filenames.\n",
      "Found 344 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_20          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_21          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_22          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_23          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_24          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_4      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚         \u001b[38;5;34m1,792\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_20          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_35 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_36 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_21          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_36 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_37 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚       \u001b[38;5;34m295,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_22          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_37 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_38 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)      â”‚     \u001b[38;5;34m1,180,160\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_23          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)      â”‚         \u001b[38;5;34m2,048\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_38 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_39 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      â”‚     \u001b[38;5;34m2,359,808\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_24          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      â”‚         \u001b[38;5;34m2,048\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_39 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_4      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,916,674</span> (14.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,916,674\u001b[0m (14.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,913,728</span> (14.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,913,728\u001b[0m (14.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/29 10:43:07 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a63ef2d7948d4accba0011b0a6072247', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "2025/11/29 10:43:07 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'keras.src.legacy.preprocessing.image.DataFrameIterator'>. Dataset logging skipped.\n",
      "2025/11/29 10:43:07 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'keras.src.legacy.preprocessing.image.DataFrameIterator'>. Dataset logging skipped.\n",
      "c:\\Users\\LamdaZero\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545ms/step - binary_accuracy: 0.6978 - loss: 0.9927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - binary_accuracy: 0.7634 - loss: 0.8848 - val_binary_accuracy: 0.8875 - val_loss: 0.3507 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - binary_accuracy: 0.8438 - loss: 0.5050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LamdaZero\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - binary_accuracy: 0.8438 - loss: 0.5050 - val_binary_accuracy: 0.8844 - val_loss: 0.3547 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - binary_accuracy: 0.7943 - loss: 0.5701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 821ms/step - binary_accuracy: 0.8127 - loss: 0.5560 - val_binary_accuracy: 0.9125 - val_loss: 0.2732 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - binary_accuracy: 0.8750 - loss: 0.4045"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 392ms/step - binary_accuracy: 0.8750 - loss: 0.4045 - val_binary_accuracy: 0.9125 - val_loss: 0.2667 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step - binary_accuracy: 0.8755 - loss: 0.4279"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 825ms/step - binary_accuracy: 0.8694 - loss: 0.4243 - val_binary_accuracy: 0.9125 - val_loss: 0.2263 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - binary_accuracy: 0.8750 - loss: 0.4010\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 383ms/step - binary_accuracy: 0.8750 - loss: 0.4010 - val_binary_accuracy: 0.9094 - val_loss: 0.2202 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 536ms/step - binary_accuracy: 0.8821 - loss: 0.3684 - val_binary_accuracy: 0.9125 - val_loss: 0.2214 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - binary_accuracy: 0.8750 - loss: 0.2639 - val_binary_accuracy: 0.9094 - val_loss: 0.2225 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - binary_accuracy: 0.9026 - loss: 0.5176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 844ms/step - binary_accuracy: 0.9000 - loss: 0.5343 - val_binary_accuracy: 0.9156 - val_loss: 0.2023 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - binary_accuracy: 0.9688 - loss: 0.3055 - val_binary_accuracy: 0.9125 - val_loss: 0.2051 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - binary_accuracy: 0.8882 - loss: 0.8152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 839ms/step - binary_accuracy: 0.8955 - loss: 0.5054 - val_binary_accuracy: 0.9156 - val_loss: 0.1978 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - binary_accuracy: 0.7812 - loss: 0.4514"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 404ms/step - binary_accuracy: 0.7812 - loss: 0.4514 - val_binary_accuracy: 0.9250 - val_loss: 0.1874 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - binary_accuracy: 0.8861 - loss: 1.4735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 835ms/step - binary_accuracy: 0.8963 - loss: 0.7442 - val_binary_accuracy: 0.9250 - val_loss: 0.1765 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - binary_accuracy: 0.8125 - loss: 0.3820 - val_binary_accuracy: 0.9219 - val_loss: 0.1818 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - binary_accuracy: 0.8935 - loss: 0.3308\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 539ms/step - binary_accuracy: 0.8985 - loss: 0.3126 - val_binary_accuracy: 0.9094 - val_loss: 0.1901 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - binary_accuracy: 0.8438 - loss: 0.3214 - val_binary_accuracy: 0.9094 - val_loss: 0.1922 - learning_rate: 2.5000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 540ms/step - binary_accuracy: 0.9037 - loss: 0.3621 - val_binary_accuracy: 0.9125 - val_loss: 0.1858 - learning_rate: 2.5000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - binary_accuracy: 0.8438 - loss: 0.3087\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - binary_accuracy: 0.8438 - loss: 0.3087 - val_binary_accuracy: 0.9156 - val_loss: 0.1827 - learning_rate: 2.5000e-04\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "ğŸƒ View run gaudy-shrike-23 at: https://dagshub.com/carlosfcfm/Experimentos.mlflow/#/experiments/22/runs/a63ef2d7948d4accba0011b0a6072247\n",
      "ğŸ§ª View experiment at: https://dagshub.com/carlosfcfm/Experimentos.mlflow/#/experiments/22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Rutas\n",
    "base_dir = r'C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES'\n",
    "img_dir = os.path.join(base_dir, 'img_align_celeba')\n",
    "augmented_dir = os.path.join(base_dir, 'augmented_my_faces')\n",
    "attr_csv = os.path.join(base_dir, 'list_attr_celeba.csv')\n",
    "partition_csv = os.path.join(base_dir, 'list_eval_partition.csv')\n",
    "pretrained_model_path = os.path.join(base_dir, 'celeba_attribute_model.h5')  \n",
    "\n",
    "# ------- 1. Contamos positivos (imagenes que si son de mi rostro) ----------------------\n",
    "positive_files = [f for f in os.listdir(augmented_dir) if f.endswith(('.jpeg', '.jpg', '.png'))]\n",
    "n_positives = len(positive_files)\n",
    "n_negatives = n_positives * 2  # Hacemos balance 1:2, teniendo el doble de negativos (imagenes que no son de mi rostro) que de positivos\n",
    "\n",
    "\n",
    "# ----------- 2. Preparamos negativos con caracterÃ­sticas similares ----------------- #\n",
    "# Cargamos atributos y particiones\n",
    "attributes = pd.read_csv(attr_csv)\n",
    "partitions = pd.read_csv(partition_csv)\n",
    "\n",
    "# Filtramos solo el dataset de train (partition==0) y unimos atributos\n",
    "train_part = partitions[partitions['partition'] == 0]\n",
    "train_data = train_part.merge(attributes, on='image_id')\n",
    "\n",
    "# Mapeamos -1 a 0 para consistencia\n",
    "attr_columns = train_data.columns[2:]  # Atributos\n",
    "train_data[attr_columns] = (train_data[attr_columns] + 1) // 2  \n",
    "\n",
    "# Definimos filtros basados en los atributos de mi rostro\n",
    "attr_filters = {\n",
    "    'Male': 1,          \n",
    "    'Young': 1,         \n",
    "    'Eyeglasses': 1,    \n",
    "    'Black_Hair': 1,    \n",
    "    'No_Beard': 1       \n",
    "}\n",
    "\n",
    "# Filtramos negativos similares\n",
    "similar_negatives = train_data.copy()\n",
    "for attr, value in attr_filters.items():\n",
    "    similar_negatives = similar_negatives[similar_negatives[attr] == value]\n",
    "\n",
    "print(f\"NÃºmero de negativos similares disponibles: {len(similar_negatives)}\")\n",
    "\n",
    "# Sampleamos similares y si no hay, ocupamos aleatorios extra\n",
    "if len(similar_negatives) < n_negatives:\n",
    "    print(\"No hay suficientes similaresâ€”usando todos + aleatorios extras\")\n",
    "    train_negatives = similar_negatives\n",
    "    extras_needed = n_negatives - len(similar_negatives)\n",
    "    if extras_needed > 0:\n",
    "        extras = train_data[~train_data['image_id'].isin(similar_negatives['image_id'])].sample(extras_needed)\n",
    "        train_negatives = pd.concat([train_negatives, extras])\n",
    "else:\n",
    "    train_negatives = similar_negatives.sample(n_negatives)\n",
    "\n",
    "# ------------ 3. Creamos dataframes ----------------------------------------------\n",
    "positive_df = pd.DataFrame({'filename': positive_files, 'label': 1.0})\n",
    "positive_df['path'] = augmented_dir + '\\\\' + positive_df['filename']\n",
    "\n",
    "negative_df = pd.DataFrame({'filename': train_negatives['image_id'], 'label': 0.0})\n",
    "negative_df['path'] = img_dir + '\\\\' + negative_df['filename']\n",
    "\n",
    "all_data = pd.concat([positive_df, negative_df[['filename', 'label', 'path']]], ignore_index=True)\n",
    "train_df, val_df = train_test_split(all_data, test_size=0.2, stratify=all_data['label'], random_state=42)\n",
    "\n",
    "print(\"DistribuciÃ³n train:\", train_df['label'].value_counts().to_dict())\n",
    "print(\"DistribuciÃ³n val:  \", val_df['label'].value_counts().to_dict())\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.6, 1.4],\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, x_col='path', y_col='label',\n",
    "    target_size=(96, 96),  \n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df, x_col='path', y_col='label',\n",
    "    target_size=(96, 96),\n",
    "    batch_size=32,\n",
    "    class_mode='raw'\n",
    ")\n",
    "\n",
    "# --------------- 5. Cargamos y modificamos el modelo -------------------------------------\n",
    "model = load_model(pretrained_model_path)\n",
    "model.build((None, 96, 96, 3))  \n",
    "\n",
    "# Eliminamos las capas finales (son las capas del clasificador)\n",
    "model.pop()  # Ãšltima dense (40)\n",
    "model.pop()  # Dropout\n",
    "model.pop()  # Dense intermedia\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# AÃ±adimos el nuevo clasificador\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Congelamos las capas base\n",
    "for layer in model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compilar\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']  \n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_binary_accuracy', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Usamos class weight para priorizar positivos \n",
    "class_weight = {0: 1.0, 1: 1.5}  # Le damos un poco de mÃ¡s peso a positivos\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=max(1, len(train_df) // 32),\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=max(1, len(val_df) // 32),\n",
    "    epochs=30,\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "model.save(os.path.join(base_dir, 'face_recognizer_frozen_similar.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d576bd",
   "metadata": {},
   "source": [
    "## PARTE 4: FINE-TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf9597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025/11/29 10:50:02 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0b870bff6a9c45fba20f9b78c88a1ca8', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n",
      "2025/11/29 10:50:03 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'keras.src.legacy.preprocessing.image.DataFrameIterator'>. Dataset logging skipped.\n",
      "2025/11/29 10:50:03 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'keras.src.legacy.preprocessing.image.DataFrameIterator'>. Dataset logging skipped.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - accuracy: 0.8316 - loss: 0.3300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 841ms/step - accuracy: 0.8709 - loss: 0.2834 - val_accuracy: 0.9125 - val_loss: 0.2190 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - accuracy: 0.9688 - loss: 0.2078 - val_accuracy: 0.8969 - val_loss: 0.2276 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 570ms/step - accuracy: 0.8925 - loss: 0.2521 - val_accuracy: 0.9000 - val_loss: 0.2266 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - accuracy: 0.9375 - loss: 0.1858\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 401ms/step - accuracy: 0.9375 - loss: 0.1858 - val_accuracy: 0.9094 - val_loss: 0.2085 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.8879 - loss: 0.2464"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 826ms/step - accuracy: 0.8910 - loss: 0.2455 - val_accuracy: 0.9125 - val_loss: 0.1963 - learning_rate: 5.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - accuracy: 0.8750 - loss: 0.2481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 410ms/step - accuracy: 0.8750 - loss: 0.2481 - val_accuracy: 0.9094 - val_loss: 0.1906 - learning_rate: 5.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.9165 - loss: 0.2456"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 837ms/step - accuracy: 0.9082 - loss: 0.2389 - val_accuracy: 0.9156 - val_loss: 0.1830 - learning_rate: 5.0000e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m7s\u001b[0m 175ms/step - accuracy: 0.8750 - loss: 0.3398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 401ms/step - accuracy: 0.8750 - loss: 0.3398 - val_accuracy: 0.9187 - val_loss: 0.1829 - learning_rate: 5.0000e-05\n",
      "Epoch 9/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.9007 - loss: 0.2457"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 838ms/step - accuracy: 0.9000 - loss: 0.2383 - val_accuracy: 0.9187 - val_loss: 0.1771 - learning_rate: 5.0000e-05\n",
      "Epoch 10/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.9375 - loss: 0.1866 - val_accuracy: 0.9187 - val_loss: 0.1773 - learning_rate: 5.0000e-05\n",
      "Epoch 11/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.9104 - loss: 0.2258"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 843ms/step - accuracy: 0.9075 - loss: 0.2284 - val_accuracy: 0.9250 - val_loss: 0.1672 - learning_rate: 5.0000e-05\n",
      "Epoch 12/15\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - accuracy: 0.8750 - loss: 0.2490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 390ms/step - accuracy: 0.8750 - loss: 0.2490 - val_accuracy: 0.9250 - val_loss: 0.1623 - learning_rate: 5.0000e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 550ms/step - accuracy: 0.9075 - loss: 0.2273 - val_accuracy: 0.9187 - val_loss: 0.1727 - learning_rate: 5.0000e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - accuracy: 0.7500 - loss: 0.4385\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.7500 - loss: 0.4385 - val_accuracy: 0.9156 - val_loss: 0.1742 - learning_rate: 5.0000e-05\n",
      "Epoch 15/15\n",
      "\u001b[1m42/42\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 564ms/step - accuracy: 0.9104 - loss: 0.2374 - val_accuracy: 0.9250 - val_loss: 0.1684 - learning_rate: 2.5000e-05\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "ğŸƒ View run invincible-ray-90 at: https://dagshub.com/carlosfcfm/Experimentos.mlflow/#/experiments/22/runs/0b870bff6a9c45fba20f9b78c88a1ca8\n",
      "ğŸ§ª View experiment at: https://dagshub.com/carlosfcfm/Experimentos.mlflow/#/experiments/22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completado y modelo guardado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "base_dir = r'C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES'\n",
    "frozen_model_path = os.path.join(base_dir, 'face_recognizer_frozen_similar.h5')\n",
    "\n",
    "model = load_model(frozen_model_path)\n",
    "\n",
    "\n",
    "for layer in model.layers[-6:]:  \n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_df) // 32,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_df) // 32,\n",
    "    epochs=15,\n",
    "    callbacks = [reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "\n",
    "model.save(os.path.join(base_dir, 'face_recognizer_finetuned.h5'))\n",
    "print(\"Fine-tuning completado y modelo guardado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ac3f8",
   "metadata": {},
   "source": [
    "## PARTE 5: CREAMOS UN DATASET DE TEST, DONDE TENGA IMAGENES DE MI ROSTRO (67) E IMAGENES QUE NO SON DE MI ROSTRO (67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b25a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de test preparado con 67 positivos y 67 negativos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "base_dir = r'C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES'\n",
    "img_dir = os.path.join(base_dir, 'img_align_celeba')\n",
    "test_dir = os.path.join(base_dir, 'test_faces')\n",
    "positive_dir = os.path.join(test_dir, 'positive')\n",
    "negative_dir = os.path.join(test_dir, 'negative')\n",
    "os.makedirs(positive_dir, exist_ok=True)\n",
    "os.makedirs(negative_dir, exist_ok=True)\n",
    "\n",
    "for f in os.listdir(test_dir):\n",
    "    if f.endswith('.jpg') and os.path.isfile(os.path.join(test_dir, f)):\n",
    "        shutil.move(os.path.join(test_dir, f), os.path.join(positive_dir, f))\n",
    "\n",
    "# Cargamos CSVs para negativos de CelebA\n",
    "attr_csv = os.path.join(base_dir, 'list_attr_celeba.csv')\n",
    "partition_csv = os.path.join(base_dir, 'list_eval_partition.csv')\n",
    "attributes = pd.read_csv(attr_csv)\n",
    "partitions = pd.read_csv(partition_csv)\n",
    "data = attributes.merge(partitions, on='image_id')\n",
    "\n",
    "attr_columns = data.columns[1:41]\n",
    "data[attr_columns] = (data[attr_columns] + 1) // 2\n",
    "\n",
    "# Filtramos la particiÃ³n de test (partition==2) para negativos no vistos\n",
    "test_data = data[data['partition'] == 2]\n",
    "\n",
    "# Usamos filtros similares a mis atributos\n",
    "attr_filters = {\n",
    "    'Male': 1,\n",
    "    'Young': 1,\n",
    "    'Eyeglasses': 1,\n",
    "    'Black_Hair': 1,\n",
    "    'No_Beard': 1\n",
    "}\n",
    "\n",
    "similar_negatives = test_data.copy()\n",
    "for attr, value in attr_filters.items():\n",
    "    similar_negatives = similar_negatives[similar_negatives[attr] == value]\n",
    "\n",
    "n_negatives = 67  \n",
    "if len(similar_negatives) < n_negatives:\n",
    "    negatives = similar_negatives\n",
    "    extras_needed = n_negatives - len(negatives)\n",
    "    extras = test_data[~test_data['image_id'].isin(similar_negatives['image_id'])].sample(extras_needed)\n",
    "    negatives = pd.concat([negatives, extras])\n",
    "else:\n",
    "    negatives = similar_negatives.sample(n_negatives)\n",
    "\n",
    "# Copiar negativos a la carpeta negative_dir\n",
    "for img_id in negatives['image_id']:\n",
    "    src = os.path.join(img_dir, img_id)\n",
    "    dst = os.path.join(negative_dir, img_id)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset de test preparado con 67 positivos y 67 negativos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f9c21",
   "metadata": {},
   "source": [
    "## PARTE 6: MEDIMOS LA CONFIANZA DEL MODELO Y VEMOS EL SCORE EN AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131d3e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LamdaZero\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 543ms/step\n",
      "Reporte de clasificaciÃ³n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       No yo       0.98      0.93      0.95        67\n",
      "          Yo       0.93      0.99      0.96        67\n",
      "\n",
      "    accuracy                           0.96       134\n",
      "   macro avg       0.96      0.96      0.96       134\n",
      "weighted avg       0.96      0.96      0.96       134\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9953218979728224\n",
      "Imagen 0: Probabilidad de ser yo: 0.92, Label real: 1.0\n",
      "Imagen 1: Probabilidad de ser yo: 0.84, Label real: 1.0\n",
      "Imagen 2: Probabilidad de ser yo: 0.93, Label real: 1.0\n",
      "Imagen 3: Probabilidad de ser yo: 0.91, Label real: 1.0\n",
      "Imagen 4: Probabilidad de ser yo: 0.93, Label real: 1.0\n",
      "Imagen 5: Probabilidad de ser yo: 0.95, Label real: 1.0\n",
      "Imagen 6: Probabilidad de ser yo: 0.93, Label real: 1.0\n",
      "Imagen 7: Probabilidad de ser yo: 0.92, Label real: 1.0\n",
      "Imagen 8: Probabilidad de ser yo: 0.93, Label real: 1.0\n",
      "Imagen 9: Probabilidad de ser yo: 0.88, Label real: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "base_dir = r'C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES'\n",
    "model_path = os.path.join(base_dir, 'face_recognizer_finetuned.h5')  \n",
    "test_dir = os.path.join(base_dir, 'test_faces')  # Carpeta con subfolders positivas/negativas\n",
    "\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Preparamos el dataframe de test \n",
    "positive_test_files = [os.path.join(test_dir, 'positive', f) for f in os.listdir(os.path.join(test_dir, 'positive')) if f.endswith(('.jpg', '.jpeg'))]\n",
    "negative_test_files = [os.path.join(test_dir, 'negative', f) for f in os.listdir(os.path.join(test_dir, 'negative')) if f.endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'path': positive_test_files + negative_test_files,\n",
    "    'label': [1.0] * len(positive_test_files) + [0.0] * len(negative_test_files)\n",
    "})\n",
    "\n",
    "# Generador para test (sin augmentaciÃ³n)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(96, 96),  \n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=False \n",
    ")\n",
    "\n",
    "# Predecimos en test\n",
    "predictions = model.predict(test_generator)\n",
    "y_pred = (predictions > 0.5).astype(int).flatten() \n",
    "y_true = test_df['label'].values\n",
    "\n",
    "# MÃ©tricas \n",
    "print(\"Reporte de clasificaciÃ³n:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['No yo', 'Yo']))\n",
    "\n",
    "# Matriz de confusiÃ³n\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels = ['No yo', 'yo']\n",
    "plt.figure(figsize=(8, 6))  \n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',  \n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar=True, square=True, linewidths=0.5)\n",
    "\n",
    "plt.title('Matriz de ConfusiÃ³n - Reconocimiento Facial', fontsize=16)\n",
    "plt.xlabel('PredicciÃ³n del Modelo', fontsize=12)\n",
    "plt.ylabel('Etiqueta Verdadera', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# AUC-ROC\n",
    "auc = roc_auc_score(y_true, predictions)\n",
    "print(\"AUC-ROC:\", auc)\n",
    "\n",
    "\n",
    "# Para confianza por imagen (elegimos 10 ejemplos)\n",
    "for i, pred in enumerate(predictions[:10]):  \n",
    "    print(f\"Imagen {i}: Probabilidad de ser yo: {pred[0]:.2f}, Label real: {y_true[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670381c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 493ms/step - accuracy: 0.9552 - loss: 0.1305\n",
      "Loss en test: 0.1304597109556198\n",
      "Accuracy en test: 0.9552238583564758\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "base_dir = r'C:\\Users\\LamdaZero\\Desktop\\Proyecto final REDES'\n",
    "model_path = os.path.join(base_dir, 'face_recognizer_finetuned.h5')\n",
    "\n",
    "\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "score = model.evaluate(test_generator, verbose=1)\n",
    "print(\"Loss en test:\", score[0])\n",
    "print(\"Accuracy en test:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
